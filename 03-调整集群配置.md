# 实验（三）调整集群规格和配置

## 一、调整集群大小

前一步实验创建的集群时候没有指定节点数量，默认是2个节点，且系统会自动生成nodegroup，并分配nodegroup id。下面对这个nodegroup做扩容。

查询刚才集群的nodegroup id，执行如下命令，请替换集群名称为本次实验的名称。

```
eksctl get nodegroup --cluster eksworkshop
```

输出结果如下。

```
CLUSTER		NODEGROUP	CREATED			MIN SIZE	MAX SIZE	DESIRED CAPACITY	INSTANCE TYPE	IMAGE ID
eksworkshop	ng-139007c2	2020-08-02T02:18:39Z	2		2		2			t3.medium
```

这里的 ng-139007c2 就是nodegroup的id，可以看到目前是最大2个节点，期望值是2个节点。

这个过程也可以使用json直接格式化输出，例如执行如下语句。

```
eksctl get nodegroup --cluster eksworkshop --region=cn-northwest-1 -o json | jq -r '.[].Name'
```

现在扩展集群到3个节点，并设置最大6节点。

```
eksctl scale nodegroup --cluster=eksworkshop --nodes-max=6 --nodes=3 --name=ng-139007c2 --region=cn-northwest-1
```

执行结果如下表示扩展成功。

```
[ℹ]  scaling nodegroup stack "eksctl-eksworkshop-nodegroup-ng-139007c2" in cluster eksctl-eksworkshop-cluster
[ℹ]  scaling nodegroup, desired capacity from 2 to 3, max size from 2 to 6
```

执行如下命令检查node数量。

```
kubectl get node                                                                                                                                                                                ```

返回结果如下表示扩容到3节点成功。

```
NAME                                                STATUS   ROLES    AGE   VERSION
ip-192-168-37-88.cn-northwest-1.compute.internal    Ready    <none>   9d    v1.17.9-eks-4c6976
ip-192-168-68-157.cn-northwest-1.compute.internal   Ready    <none>   30s   v1.17.9-eks-4c6976
ip-192-168-7-107.cn-northwest-1.compute.internal    Ready    <none>   9d    v1.17.9-eks-4c6976
```

此时可以再次查询节点属性，执行如下命令。

```
eksctl get nodegroup --cluster eksworkshop
```

返回结果如下表示扩容成功。

```
CLUSTER		NODEGROUP	CREATED			MIN SIZE	MAX SIZE	DESIRED CAPACITY	INSTANCE TYPE	IMAGE ID
eksworkshop	ng-139007c2	2020-08-02T02:18:39Z	2		6		3			t3.medium
```

## 二、调整集群EC2规格

前文创建集群使用的是t3.medium规格，2vCPU/4GB内存，EKS不支持对现有的nodegroup修改配置。因此，为了规格，例如更换为t3a.large，需要在同一个cluster集群下，新建一个nodegroup，并使用新规格。随后在删除旧的nodegroup，pod也将会自动在新nodegroup上拉起。整个过程不影响应用访问。

### 1、新建使用新的EC2规格的nodegroup

执行如下命令新建一个nodegroup。

```
eksctl create nodegroup \
--cluster eksworkshop \
--version 1.17 \
--name eskdemo \
--node-type t3a.large \
--nodes 3 \
--nodes-min 3 \
--nodes-max 6 \
--tags Name=eksnode \
--managed
```

执行结果如下。

```
[ℹ]  eksctl version 0.25.0
[ℹ]  using region cn-northwest-1
[ℹ]  nodegroup "newec2" present in the given config, but missing in the cluster
[ℹ]  nodegroup "ng-139007c2" present in the cluster, but missing from the given config
[ℹ]  1 existing nodegroup(s) (ng-139007c2) will be excluded
[ℹ]  nodegroup "newec2" will use "ami-09be27ee84f66b04f" [AmazonLinux2/1.17]
[ℹ]  1 nodegroup (newec2) was included (based on the include/exclude rules)
[ℹ]  will create a CloudFormation stack for each of 1 nodegroups in cluster "eksworkshop"
[ℹ]  2 sequential tasks: { fix cluster compatibility, 1 task: { 1 task: { create nodegroup "newec2" } } }
[ℹ]  checking cluster stack for missing resources
[ℹ]  cluster stack has all required resources
[ℹ]  building nodegroup stack "eksctl-eksworkshop-nodegroup-newec2"
[ℹ]  deploying stack "eksctl-eksworkshop-nodegroup-newec2"
[ℹ]  no tasks
[ℹ]  adding identity "arn:aws-cn:iam::420029960748:role/eksctl-eksworkshop-nodegroup-newe-NodeInstanceRole-1GH5I5034YU7O" to auth ConfigMap
[ℹ]  nodegroup "newec2" has 0 node(s)
[ℹ]  waiting for at least 3 node(s) to become ready in "newec2"
[ℹ]  nodegroup "newec2" has 3 node(s)
[ℹ]  node "ip-192-168-3-67.cn-northwest-1.compute.internal" is ready
[ℹ]  node "ip-192-168-42-229.cn-northwest-1.compute.internal" is ready
[ℹ]  node "ip-192-168-92-35.cn-northwest-1.compute.internal" is ready
[✔]  created 1 nodegroup(s) in cluster "eksworkshop"
[✔]  created 0 managed nodegroup(s) in cluster "eksworkshop"
[ℹ]  checking security group configuration for all nodegroups
[ℹ]  all nodegroups have up-to-date configuration
```

执行如下命令查看nodegroup是否正常。

```
eksctl get nodegroup --cluster eksworkshop
```

返回结果可以看到原来的和新创建的两个nodegroup如下。

```
CLUSTER		NODEGROUP	CREATED			MIN SIZE	MAX SIZE	DESIRED CAPACITY	INSTANCE TYPE	IMAGE ID
eksworkshop	newec2		2020-08-12T03:50:28Z	3		6		3			t3a.large	ami-09be27ee84f66b04f
eksworkshop	ng-139007c2	2020-08-02T02:18:39Z	2		6		3			t3.medium
```

### 2、删除旧的nodegroup

执行如下命令。

```
eksctl delete nodegroup --cluster eksworkshop --name ng-139007c2
```

执行效果如下。此时通过输出可以看到，EKS将旧的nodegroup设置为 "drained" 状态，并从集群中删除。

```
[ℹ]  eksctl version 0.25.0
[ℹ]  using region cn-northwest-1
[ℹ]  1 nodegroup (ng-139007c2) was included (based on the include/exclude rules)
[ℹ]  will drain 1 nodegroup(s) in cluster "eksworkshop"
[ℹ]  cordon node "ip-192-168-37-88.cn-northwest-1.compute.internal"
[ℹ]  cordon node "ip-192-168-68-157.cn-northwest-1.compute.internal"
[ℹ]  cordon node "ip-192-168-7-107.cn-northwest-1.compute.internal"
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-6p6h9, kube-system/kube-proxy-pq2c4
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-fgldc, kube-system/kube-proxy-7ht8n
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-xzgls, kube-system/kube-proxy-zxs6g
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-6p6h9, kube-system/kube-proxy-pq2c4
[!]  ignoring DaemonSet-managed Pods: kube-system/aws-node-xzgls, kube-system/kube-proxy-zxs6g
[✔]  drained nodes: [ip-192-168-37-88.cn-northwest-1.compute.internal ip-192-168-68-157.cn-northwest-1.compute.internal ip-192-168-7-107.cn-northwest-1.compute.internal]
[ℹ]  will delete 1 nodegroups from cluster "eksworkshop"
[ℹ]  1 task: { delete nodegroup "ng-139007c2" [async] }
[ℹ]  will delete stack "eksctl-eksworkshop-nodegroup-ng-139007c2"
[ℹ]  will delete 0 nodegroups from auth ConfigMap in cluster "eksworkshop"
[✔]  deleted 1 nodegroup(s) from cluster "eksworkshop"

```

现实当前集群nodegroup，执行如下命令。

```
eksctl get nodegroup --cluster eksworkshop
```

返回结果如下。

```
CLUSTER		NODEGROUP	CREATED			MIN SIZE	MAX SIZE	DESIRED CAPACITY	INSTANCE TYPE	IMAGE ID
eksworkshop	newec2		2020-08-12T03:50:28Z	3		6		3			t3a.large	ami-09be27ee84f66b04f
```

通过以上返回结果可以看到，原来使用t3.medium规格的nodegroup已经被删除，集群中新的nodegroup是使用t3a.large规格的节点。

此时如果集群中有应用运行，将自动在新的nodegroup中拉起pod运行，更换nodegroup的过程不影响应用运行。